{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Google Forever"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This cannot be done locally. We need to go to cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SQS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Basic workflow of Amazon SQS](https://do38he9wic6d8.cloudfront.net/sqsconsole-20220308192726570/assets/images/5e3f44ce52788a4fb8b8432e4441bf3f-SQS-diagram.svg \"Basic workflow of Amazon SQS\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are 3 main components\n",
    "    * Producers - The queries we auto generate locally.\n",
    "    * SQS Queue - Those queries will get pushed to SQS Queue\n",
    "    * Consumers - Lambda function will process each of these queues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We have covered this in the last post. Run the auto-generate key in an infinite loop. ( Which is plain crazy ). But it will prove our point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Amazon SQS provides queues for high-throughput, system-to-system messaging. You can use queues to decouple heavyweight processes and to buffer and batch work. Amazon SQS stores messages until microservices and serverless applications process them."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SQS is simple. You gave it tasks to do. All the tasks are going to be pushed into a queue. The queue then distributes the task to microservices or Lambda functions or EC2 instances to process. So we are gonna create a queue and push the messages(tasks) from the producer to a Amazon SQS queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It's simple. Go to AWS console and log into your account. (AWS account is a prerequisite here). Then search for SQS. Then select it.\n",
    "There are two options to create a queue. One is standard and other is FIFO."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The order doesn't matter to us. We just want to scrape infinitely. But it is important for us that we process every request. The standard one is more suitable for us in this regard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQS Configuration"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Visibility timeout : The consumer (lambda function) might try to process it and fail with an error message 'max tries exceeded'. If the consumer fails we want other instances of consumer to pick it up. Documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The visibility timeout begins when Amazon SQS returns a message. If the consumer fails to process and delete the message before the visibility timeout expires, the message becomes visible to other consumers. If a message must be received only once, your consumer must delete it within the duration of the visibility timeout."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "That means if we set it to a value like an hour, the consumer would fail in that timeframe if it has to fail. In that scenario the message won't be lost and we want other consumer instances to pick it up again and try. That raises another important question. How many times do we retry a task. And how do we configure this. In comes ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dead Letter Queue (DLQ)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This queue is like a normal queue. Except it recieves all the dead messages that the original queue (source queue) can't process even after trying 'n' times. That number of retries 'n' is defined in 'Maximum recieves' as shown below. So the source queue will try to process the message 10 times before sending it to the dead letter queue. We can use this queue for determining why we can't process a certain message. All the source queues we build can have the same DLQ queue given the type of both source and DLQ are same. (Either standard or FIFO)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we can click that create button to create our second queue (We already created a DLQ). Hurray!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We can create a queue using the boto3 AWS python SDK. Sample code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_queue():\n",
    "    sqs_client = boto3.client(\"sqs\", region_name=\"us-west-2\")\n",
    "    response = sqs_client.create_queue(\n",
    "        QueueName=\"my-new-queue\",\n",
    "        Attributes={\n",
    "            \"DelaySeconds\": \"0\",\n",
    "            \"VisibilityTimeout\": \"60\",  # 60 seconds\n",
    "        }\n",
    "    )\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce Messages"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We have set up the queue and it's time to send messages to the queue. We will use the boto3 module to send message to SQS from a python module running in our local machine or an EC2 instance. The code looks like below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "QueueDoesNotExist",
     "evalue": "An error occurred (AWS.SimpleQueueService.NonExistentQueue) when calling the SendMessage operation: The specified queue does not exist for this wsdl version.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mQueueDoesNotExist\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-68d478715612>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Send message to SQS queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m response = sqs.send_message(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mQueueUrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mDelaySeconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mQueueDoesNotExist\u001b[0m: An error occurred (AWS.SimpleQueueService.NonExistentQueue) when calling the SendMessage operation: The specified queue does not exist for this wsdl version."
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Create SQS client\n",
    "sqs = boto3.client('sqs')\n",
    "\n",
    "queue_url =  'SQS_QUEUE_URL'\n",
    "\n",
    "# Send message to SQS queue\n",
    "response = sqs.send_message(\n",
    "    QueueUrl=queue_url,\n",
    "    DelaySeconds=10,\n",
    "    MessageAttributes={\n",
    "        'Title': {\n",
    "            'DataType': 'String',\n",
    "            'StringValue': 'The Whistler'\n",
    "        },\n",
    "        'Author': {\n",
    "            'DataType': 'String',\n",
    "            'StringValue': 'John Grisham'\n",
    "        },\n",
    "        'WeeksOn': {\n",
    "            'DataType': 'Number',\n",
    "            'StringValue': '6'\n",
    "        }\n",
    "    },\n",
    "    MessageBody=(\n",
    "        'Information about current NY Times fiction bestseller for '\n",
    "        'week of 12/11/2016.'\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response['MessageId'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Only the queue_url and the MessageBody is required. MessageAttributes is useful if we want to consume a message in a different way in a queue. For (a very typical) example, if you have json and xml type MessageBody when you produce the messages and you have two processor methods in the consumer, one for processing json message and one for xml, the MessageAttributes passed alongside the message helps in deciding which processor to use for an incoming message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.21.23-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 589 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 497 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
      "Collecting botocore<1.25.0,>=1.24.23\n",
      "  Downloading botocore-1.24.23-py3-none-any.whl (8.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.6 MB 322 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/jo/opt/anaconda3/lib/python3.8/site-packages (from botocore<1.25.0,>=1.24.23->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/jo/opt/anaconda3/lib/python3.8/site-packages (from botocore<1.25.0,>=1.24.23->boto3) (1.26.9)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jo/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.23->boto3) (1.15.0)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.21.23 botocore-1.24.23 jmespath-1.0.0 s3transfer-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a very good idea to flaunt your AWS access key and secret key. It's better to confiqure AWS using [AWSCLI](https://aws.amazon.com/cli/)\n",
    "<br>\n",
    "Once you configure AWS on your system you can pass the required credentials through os.environ variables like mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "os.environ['AWS_PROFILE'] = \"default\"\n",
    "os.environ['AWS_DEFAULT_REGION'] = \"us-west-1\"\n",
    "\n",
    "# Create SQS client\n",
    "sqs = boto3.client(\"sqs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue_url = 'https://sqs.us-west-1.amazonaws.com/XXXXXXXXXXX/google_query' # 'SQS_QUEUE_URL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c4e0e462-fe87-4947-97b0-260ac0ea47fd\n"
     ]
    }
   ],
   "source": [
    "# Send message to SQS queue\n",
    "response = sqs.send_message(\n",
    "    QueueUrl=queue_url,\n",
    "    DelaySeconds=10,\n",
    "    MessageBody=(\n",
    "        'Query Google infinitely'\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response['MessageId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our first message added to the queue."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now I'm not crazy to scrape google infinitely nor do I have the resource but for demo purpose, let's push say 10k messages(tasks) to the queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer : Lambda function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We need to do some setting up before we can work on the lambda"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1) Wrap the scrape function in a python file\n",
    "2) Pip install all the required modules, in this case only the requests module, in the current directory\n",
    "3) Zip the entire folder with the python script and the python dependencies\n",
    "4) Create a S3 bucket to store the scraped files.\n",
    "5) Generate a new role for the s3 bucket created in the step 4 using the AWS IAM console.\n",
    "6) Create a layer in Lambda using the zip file created above. Lambda will be accessing this file.\n",
    "Once we are done with this let's create our lambda function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Go to lambda console and create function. We can create the function from scratch so choose the 'Author from scratch' option.\n",
    "\n",
    "\n",
    "Update the name, choose a python runtime, and select the role we created above from the 'existing roles' option under 'Change default execution role'. Then press the 'create function' button. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Add trigger. The trigger is the SQS queue we created earlier. The rest of the value can be kept as default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "import re\n",
    "\n",
    "class ScrapeGoogle:\n",
    "    def __init__(self):\n",
    "        self\n",
    "    # Get the source code given a url\n",
    "    def get_source(self, url):\n",
    "        # Given a url it's gonna give you the source code\n",
    "\n",
    "        try:\n",
    "            session = HTMLSession()\n",
    "            response = session.get(url)\n",
    "            return response\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "        \n",
    "        \n",
    "    def get_results(self, query):\n",
    "        # When you give a query as the input it returns the sourcecode as response\n",
    "        query = urllib.parse.quote_plus(query)\n",
    "        response = self.get_source(\"https://www.google.com/search?q=\" + query)\n",
    "        return response\n",
    "\n",
    "\n",
    "    def parse_results(self, response):\n",
    "        if not response:\n",
    "            return {}\n",
    "\n",
    "        css_identifier_result = \".tF2Cxc\"\n",
    "        css_identifier_title = \"h3\"\n",
    "        css_identifier_link = \".yuRUbf a\"\n",
    "        css_identifier_text = \".IsZvec\"\n",
    "\n",
    "        results = response.html.find(css_identifier_result)\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for result in results:\n",
    "            title = result.find(css_identifier_title, first=True)\n",
    "            title  = title.text if title is not None else \"\"\n",
    "            link = result.find(css_identifier_link, first=True)\n",
    "            link = link.attrs['href'] if link is not None else \"\"\n",
    "            text = result.find(css_identifier_text, first=True)\n",
    "            text = text.text if text is not None else \"\"\n",
    "\n",
    "            item = {\n",
    "                \"title\": title,\n",
    "                \"link\": link,\n",
    "                \"text\": text\n",
    "            }\n",
    "\n",
    "            output.append(item)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def google_search(self, query):\n",
    "        response = self.get_results(query)\n",
    "        return self.parse_results(response)\n",
    "\n",
    "    # get a valid filename out of random string\n",
    "    def get_valid_filename(self, query):\n",
    "        # Special mention : https://github.com/django/django/blob/main/django/utils/text.py\n",
    "        s = str(query).strip().replace(' ', '_')\n",
    "        return re.sub(r'(?u)[^-\\w.]', '', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = ScrapeGoogle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = scraper.google_search('aws lambda certifications')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sofmsd_salfu08q95lku_af4352436'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper.get_valid_filename('sofmsd salfu08q95lku /af4352436')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda main function\n",
    "# lambda main\n",
    "def lambda_handler(event, context):\n",
    "    \n",
    "    scraper = ScrapeGoogle()\n",
    "    bucket_name = \"google-scraped-json\"\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    \n",
    "    for record in event['Records']:\n",
    "        payload = record[\"body\"]\n",
    "        scraped_json = scraper.google_search(payload)\n",
    "        scraped_json = json.dumps({\"results\": scraped_json})\n",
    "        s3_path = \"Scraped/\" + scraper.get_valid_filename(payload[:40]) + \".json\"\n",
    "        s3.Bucket(bucket_name).put_object(Key=s3_path, Body=scraped_json)\n",
    "    return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps('file is created in:'+ s3_path)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
